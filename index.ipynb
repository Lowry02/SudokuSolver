{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from Agent import Agent, MoEAgent\n",
    "from SudokuEnvironment import SudokuEnvironment\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e439b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "is_9x9_sudoku = False\n",
    "if is_9x9_sudoku:\n",
    "  n_actions = 729\n",
    "  n_values = 10\n",
    "  input_dim = 81\n",
    "  with_legal_actions = True\n",
    "  env = SudokuEnvironment(size=9, with_legal_actions=with_legal_actions)\n",
    "else:\n",
    "  # 6x6 sudoku\n",
    "  n_actions = 216\n",
    "  n_values = 7\n",
    "  input_dim = 36\n",
    "  with_legal_actions = True\n",
    "  env = SudokuEnvironment(size=6, box_height=2, box_width=3, with_legal_actions=with_legal_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ad7ce",
   "metadata": {},
   "source": [
    "# Sparse Reward and Dense Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f83777",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_reward = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a629c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "  n_actions=n_actions,\n",
    "  n_values=n_values,\n",
    "  input_dim=input_dim,\n",
    "  train_mode=True,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 50\n",
    "n_empty_cells = 20\n",
    "n_trials = 81 if is_9x9_sudoku else 36\n",
    "sudoku_variants = 81 if is_9x9_sudoku else 81\n",
    "\n",
    "run_name = input(\"Run name: \")\n",
    "writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "agent.enable_train_mode()\n",
    "if sparse_reward:\n",
    "  env.set_sparse_reward()\n",
    "else:\n",
    "  env.set_dense_reward()\n",
    "\n",
    "for _ in range(20):\n",
    "  env.create_new()\n",
    "  agent.reset_epsilon()\n",
    "  for episode in range(n_episodes):\n",
    "    # create 81 sudoku variants starting from the same solution\n",
    "    mean_reward = 0\n",
    "    wins = 0\n",
    "    easy_wins = 0\n",
    "    mean_loss = 0\n",
    "    for sudoku_variants_id in range(sudoku_variants):\n",
    "      env.add_remove_cells(n_empty_cells)\n",
    "      n_trial_loss_sum = 0\n",
    "      trial_loss = 0\n",
    "      for trial in range(n_trials):\n",
    "        state = env.grid.unsqueeze(0).clone()\n",
    "        legal_actions = env.get_legal_actions()\n",
    "        action = agent.take_action(state, legal_actions)\n",
    "        next_state, reward, completed, win, next_legal_actions = env.step(action)\n",
    "        agent.save_experience(state.squeeze(0), legal_actions, action, reward, next_state, next_legal_actions, completed)\n",
    "        loss = agent.learn()\n",
    "        if loss != None:\n",
    "          trial_loss += loss\n",
    "          n_trial_loss_sum += 1\n",
    "        legal_actions = legal_actions.reshape(-1, 9)\n",
    "        one_valid_action_per_cell = (legal_actions.sum(dim=1) == 1).sum().item()\n",
    "        empty_cells = (state == 0).sum().item()  \n",
    "        if trial == 0 and one_valid_action_per_cell == empty_cells:\n",
    "          easy_wins += 1\n",
    "        wins += win\n",
    "        mean_reward += reward\n",
    "        if completed:\n",
    "          break\n",
    "      mean_loss += trial_loss/(n_trial_loss_sum + 1e-15)\n",
    "      env.reset()\n",
    "    mean_loss /= sudoku_variants\n",
    "    mean_reward /= sudoku_variants\n",
    "    writer.add_scalar('loss', mean_loss)\n",
    "    writer.add_scalar('wins', wins)\n",
    "    writer.add_scalar('easy_wins', easy_wins)\n",
    "    writer.add_scalar('reward', mean_reward)\n",
    "    writer.add_scalar('n_empty_cells', n_empty_cells)\n",
    "    # print(f\"[Episode {episode + 1}/{n_episodes}][{n_empty_cells}] Loss: {mean_loss:.15f} \\t Reward: {mean_reward:.1f} \\t Wins: {wins}/81 \\t Epsilon: {agent.epsilon} \\t Easy wins: {easy_wins}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64450d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(agent.behaviour_net.state_dict(), './weights/sparse_normal_learning_6x6_20.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b85de6",
   "metadata": {},
   "source": [
    "# Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d66567",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "  n_actions=n_actions,\n",
    "  n_values=n_values,\n",
    "  input_dim=input_dim,\n",
    "  train_mode=True,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 50\n",
    "n_trials = 81 if is_9x9_sudoku else 36\n",
    "sudoku_variants = 81 if is_9x9_sudoku else 81\n",
    "empty_cells_range = (2, 21)\n",
    "\n",
    "run_name = input(\"Run name: \")\n",
    "writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "loss_log = {}\n",
    "for k in range(empty_cells_range[0], empty_cells_range[1]):\n",
    "  loss_log[k] = []\n",
    "\n",
    "agent.enable_train_mode()\n",
    "\n",
    "for n_empty_cells in range(empty_cells_range[0], empty_cells_range[1]):\n",
    "  env.create_new()\n",
    "  agent.reset_epsilon()\n",
    "  for episode in range(n_episodes):\n",
    "    # create 81 sudoku variants starting from the same solution\n",
    "    mean_reward = 0\n",
    "    wins = 0\n",
    "    easy_wins = 0\n",
    "    mean_loss = 0\n",
    "    for sudoku_variants_id in range(sudoku_variants):\n",
    "      env.add_remove_cells(n_empty_cells)\n",
    "      n_trial_loss_sum = 0\n",
    "      trial_loss = 0\n",
    "      for trial in range(n_trials):\n",
    "        state = env.grid.unsqueeze(0).clone()\n",
    "        legal_actions = env.get_legal_actions()\n",
    "        action = agent.take_action(state, legal_actions)\n",
    "        next_state, reward, completed, win, next_legal_actions = env.step(action)\n",
    "        agent.save_experience(state.squeeze(0), legal_actions, action, reward, next_state, next_legal_actions, completed)\n",
    "        loss = agent.learn()\n",
    "        if loss != None:\n",
    "          trial_loss += loss\n",
    "          n_trial_loss_sum += 1\n",
    "        legal_actions = legal_actions.reshape(-1, 9)\n",
    "        one_valid_action_per_cell = (legal_actions.sum(dim=1) == 1).sum().item()\n",
    "        empty_cells = (state == 0).sum().item()  \n",
    "        if trial == 0 and one_valid_action_per_cell == empty_cells:\n",
    "          easy_wins += 1\n",
    "        wins += win\n",
    "        mean_reward += reward\n",
    "        if completed:\n",
    "          break\n",
    "      mean_loss += trial_loss/(n_trial_loss_sum + 1e-15)\n",
    "      env.reset()\n",
    "    mean_loss /= sudoku_variants\n",
    "    mean_reward /= sudoku_variants\n",
    "    loss_log[n_empty_cells].append(mean_loss)\n",
    "    writer.add_scalar('loss', mean_loss)\n",
    "    writer.add_scalar('wins', wins)\n",
    "    writer.add_scalar('easy_wins', easy_wins)\n",
    "    writer.add_scalar('reward', mean_reward)\n",
    "    writer.add_scalar('n_empty_cells', n_empty_cells)\n",
    "    # print(f\"[Episode {episode + 1}/{n_episodes}][{n_empty_cells}] Loss: {mean_loss:.15f} \\t Reward: {mean_reward:.1f} \\t Wins: {wins}/81 \\t Epsilon: {agent.epsilon} \\t Easy wins: {easy_wins}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ef6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(agent.behaviour_net.state_dict(), 'sparse_curriculum_learning_6x6_20.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082557a",
   "metadata": {},
   "source": [
    "# MoE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MoEAgent(\n",
    "  n_actions=n_actions,\n",
    "  n_values=n_values,\n",
    "  input_dim=input_dim,\n",
    "  train_mode=True,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8836900",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 25\n",
    "n_trials = 81 if is_9x9_sudoku else 36\n",
    "sudoku_variants = 81 if is_9x9_sudoku else 81\n",
    "empty_cells_range = (2, 31)\n",
    "\n",
    "run_name = input(\"Run name: \")\n",
    "writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "loss_log = {}\n",
    "for k in range(empty_cells_range[0], empty_cells_range[1]):\n",
    "  loss_log[k] = []\n",
    "\n",
    "agent.enable_train_mode()\n",
    "\n",
    "for n_empty_cells in range(empty_cells_range[0], empty_cells_range[1]):\n",
    "  env.create_new()\n",
    "  agent.reset_epsilon()\n",
    "  if n_empty_cells % agent.empty_cells_group == 0: #Â if you start from 1 change it\n",
    "    agent.clone_expert((n_empty_cells // agent.empty_cells_group) - 1)\n",
    "    agent.align_target_behaviour_nets()\n",
    "    agent.empty_buffer()\n",
    "  for _ in range(3):\n",
    "    for episode in range(n_episodes):\n",
    "      # create 81 sudoku variants starting from the same solution\n",
    "      mean_reward = 0\n",
    "      wins = 0\n",
    "      mean_loss = 0\n",
    "      for sudoku_variants_id in range(sudoku_variants):\n",
    "        env.add_remove_cells(n_empty_cells)\n",
    "        n_trial_loss_sum = 0\n",
    "        trial_loss = 0\n",
    "        # for trial in range(n_trials):\n",
    "        while n_empty_cells - (n_empty_cells % agent.empty_cells_group) <= (env.grid == 0).sum() < n_empty_cells - (n_empty_cells % agent.empty_cells_group) + agent.empty_cells_group:\n",
    "          state = env.grid.unsqueeze(0).clone()\n",
    "          legal_actions = env.get_legal_actions()\n",
    "          action = agent.take_action(state, legal_actions)\n",
    "          next_state, reward, completed, win, next_legal_actions = env.step(action)\n",
    "          agent.save_experience(state.squeeze(0), legal_actions, action, reward, next_state, next_legal_actions, completed)\n",
    "          loss = agent.learn()\n",
    "          if loss != None:\n",
    "            trial_loss += loss\n",
    "            n_trial_loss_sum += 1\n",
    "          legal_actions = legal_actions.reshape(-1, 9)\n",
    "          one_valid_action_per_cell = (legal_actions.sum(dim=1) == 1).sum().item()\n",
    "          empty_cells = (state == 0).sum().item()  \n",
    "          wins += win\n",
    "          mean_reward += reward\n",
    "          if completed:\n",
    "            break\n",
    "        mean_loss += trial_loss/(n_trial_loss_sum + 1e-15)\n",
    "        env.reset()\n",
    "      mean_loss /= sudoku_variants\n",
    "      mean_reward /= sudoku_variants\n",
    "      loss_log[n_empty_cells].append(mean_loss)\n",
    "      writer.add_scalar('loss', mean_loss)\n",
    "      writer.add_scalar('wins', wins)\n",
    "      writer.add_scalar('easy_wins', easy_wins)\n",
    "      writer.add_scalar('reward', mean_reward)\n",
    "      writer.add_scalar('n_empty_cells', n_empty_cells)\n",
    "      # print(f\"[Episode {episode + 1}/{n_episodes}][{n_empty_cells}] Loss: {mean_loss:.15f} \\t Reward: {mean_reward:.1f} \\t Wins: {wins}/81 \\t Epsilon: {agent.epsilon} \\t Easy wins: {easy_wins}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(agent.behaviour_net.state_dict(), 'moe-sparse_curriculum_learning_9x9_30.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba829a6",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ebaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MoEAgent(\n",
    "  n_actions=n_actions,\n",
    "  n_values=n_values,\n",
    "  input_dim=input_dim,\n",
    "  train_mode=True,\n",
    "  device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80861391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dict = torch.load(\"moe-sparse_curriculum_learning_9x9_30.pt\")\n",
    "agent.behaviour_net.load_state_dict(state_dict)\n",
    "agent.target_net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "adda457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.disable_train_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ff0d44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rateo: 18/100\n",
      "Easy wins: 0/100\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "wins = 0\n",
    "easy_wins = 0\n",
    "for i in range(100):\n",
    "  env.create_new()\n",
    "  env.add_remove_cells(15)\n",
    "  done = False\n",
    "  i = 0\n",
    "  while not done:\n",
    "    state = env.grid.unsqueeze(0).clone()\n",
    "    legal_actions = env.get_legal_actions()\n",
    "    action = agent.take_action(state, legal_actions)\n",
    "    next_state, reward, done, win, next_legal_actions = env.step(action)\n",
    "    legal_actions = legal_actions.reshape(-1, 9)\n",
    "    one_valid_action_per_cell = (legal_actions.sum(dim=1) == 1).sum().item()\n",
    "    empty_cells = (state == 0).sum().item()\n",
    "    if i == 0 and one_valid_action_per_cell == empty_cells:\n",
    "      easy_wins += 1\n",
    "      wins += 1\n",
    "      break\n",
    "    i += 1\n",
    "  wins += win\n",
    "  \n",
    "print(f\"Rateo: {wins}/100\")\n",
    "print(f\"Easy wins: {easy_wins}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
